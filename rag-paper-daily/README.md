# ğŸ“š RAG Paper Daily

### ğŸ“… 2025-09-19
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.16112v1">CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion</a></td><td><details><summary>å±•å¼€</summary>Repository-level code completion automatically predicts the unfinished code
based on the broader information from the repository. Recent strides in Code
Large Language Models (code LLMs) have spurred the development of
repository-level code completion methods, yielding promising results.
Nevertheless, they suffer from issues such as inappropriate query construction,
single-path code retrieval, and misalignment between code retriever and code
LLM. To address these problems, we introduce CodeRAG, a framework tailored to
identify relevant and necessary knowledge for retrieval-augmented
repository-level code completion. Its core components include log probability
guided query construction, multi-path code retrieval, and preference-aligned
BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval
demonstrate that CodeRAG significantly and consistently outperforms
state-of-the-art methods. The implementation of CodeRAG is available at
https://github.com/KDEGroup/CodeRAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºCodeRAGçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ä»“åº“çº§ä»£ç è¡¥å…¨æ–¹æ³•ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚ä¸æ°å½“çš„æŸ¥è¯¢æ„å»ºã€å•ä¸€è·¯å¾„çš„ä»£ç æ£€ç´¢ä»¥åŠä»£ç æ£€ç´¢å™¨ä¸å¤§è¯­è¨€æ¨¡å‹ä¹‹é—´çš„ä¸å¯¹é½ã€‚CodeRAGé€šè¿‡æ¦‚ç‡å¼•å¯¼çš„æŸ¥è¯¢æ„å»ºã€å¤šè·¯å¾„ä»£ç æ£€ç´¢å’Œåå¥½å¯¹é½çš„BestFité‡æ’åºç­‰æ ¸å¿ƒç»„ä»¶ï¼Œæå‡äº†æ£€ç´¢å¢å¼ºçš„ä»“åº“çº§ä»£ç è¡¥å…¨çš„æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒCodeRAGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.15883v1">RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented Image Captioning</a></td><td><details><summary>å±•å¼€</summary>Recent retrieval-augmented image captioning methods incorporate external
knowledge to compensate for the limitations in comprehending complex scenes.
However, current approaches face challenges in relation modeling: (1) the
representation of semantic prompts is too coarse-grained to capture
fine-grained relationships; (2) these methods lack explicit modeling of image
objects and their semantic relationships. To address these limitations, we
propose RACap, a relation-aware retrieval-augmented model for image captioning,
which not only mines structured relation semantics from retrieval captions, but
also identifies heterogeneous objects from the image. RACap effectively
retrieves structured relation features that contain heterogeneous visual
information to enhance the semantic consistency and relational expressiveness.
Experimental results show that RACap, with only 10.8M trainable parameters,
achieves superior performance compared to previous lightweight captioning
models.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRACapçš„å…³ç³»æ„ŸçŸ¥æ£€ç´¢å¢å¼ºå›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ä»æ£€ç´¢åˆ°çš„æè¿°ä¸­æŒ–æ˜ç»“æ„åŒ–å…³ç³»è¯­ä¹‰å¹¶è¯†åˆ«å›¾åƒä¸­çš„å¼‚æ„å¯¹è±¡ï¼Œä»¥æå‡è¯­ä¹‰ä¸€è‡´æ€§å’Œå…³ç³»è¡¨è¾¾èƒ½åŠ›ï¼Œå®éªŒæ˜¾ç¤ºå…¶åœ¨è½»é‡çº§æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.15577v1">Relevance to Utility: Process-Supervised Rewrite for RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation systems often suffer from a gap between
optimizing retrieval relevance and generative utility: retrieved documents may
be topically relevant but still lack the content needed for effective reasoning
during generation. While existing "bridge" modules attempt to rewrite the
retrieved text for better generation, we show how they fail to capture true
document utility. In this work, we propose R2U, with a key distinction of
directly optimizing to maximize the probability of generating a correct answer
through process supervision. As such direct observation is expensive, we also
propose approximating an efficient distillation pipeline by scaling the
supervision from LLMs, which helps the smaller rewriter model generalize
better. We evaluate our method across multiple open-domain question-answering
benchmarks. The empirical results demonstrate consistent improvements over
strong bridging baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºR2Uçš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿä¸­æ£€ç´¢ç›¸å…³æ€§ä¸ç”Ÿæˆæ•ˆç”¨ä¹‹é—´çš„ä¸ä¸€è‡´é—®é¢˜ã€‚é€šè¿‡ç›´æ¥ä¼˜åŒ–ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ï¼Œå¹¶åˆ©ç”¨LLMçš„ç›‘ç£ä¿¡å·æ¥é«˜æ•ˆè®­ç»ƒè¾ƒå°çš„é‡å†™æ¨¡å‹ï¼Œè®ºæ–‡åœ¨å¤šä¸ªå¼€æ”¾åŸŸé—®ç­”åŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚</details></td></tr></tbody></table>

# ğŸ“š RAG Paper Daily

### ğŸ“… 2025-09-19
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.16112v1">CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion</a></td><td><details><summary>å±•å¼€</summary>Repository-level code completion automatically predicts the unfinished code
based on the broader information from the repository. Recent strides in Code
Large Language Models (code LLMs) have spurred the development of
repository-level code completion methods, yielding promising results.
Nevertheless, they suffer from issues such as inappropriate query construction,
single-path code retrieval, and misalignment between code retriever and code
LLM. To address these problems, we introduce CodeRAG, a framework tailored to
identify relevant and necessary knowledge for retrieval-augmented
repository-level code completion. Its core components include log probability
guided query construction, multi-path code retrieval, and preference-aligned
BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval
demonstrate that CodeRAG significantly and consistently outperforms
state-of-the-art methods. The implementation of CodeRAG is available at
https://github.com/KDEGroup/CodeRAG.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºCodeRAGçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ä»“åº“çº§ä»£ç è¡¥å…¨æ–¹æ³•ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚ä¸æ°å½“çš„æŸ¥è¯¢æ„å»ºã€å•ä¸€è·¯å¾„çš„ä»£ç æ£€ç´¢ä»¥åŠä»£ç æ£€ç´¢å™¨ä¸å¤§è¯­è¨€æ¨¡å‹ä¹‹é—´çš„ä¸å¯¹é½ã€‚CodeRAGé€šè¿‡æ¦‚ç‡å¼•å¯¼çš„æŸ¥è¯¢æ„å»ºã€å¤šè·¯å¾„ä»£ç æ£€ç´¢å’Œåå¥½å¯¹é½çš„BestFité‡æ’åºç­‰æ ¸å¿ƒç»„ä»¶ï¼Œæå‡äº†æ£€ç´¢å¢å¼ºçš„ä»“åº“çº§ä»£ç è¡¥å…¨çš„æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒCodeRAGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.15883v1">RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented Image Captioning</a></td><td><details><summary>å±•å¼€</summary>Recent retrieval-augmented image captioning methods incorporate external
knowledge to compensate for the limitations in comprehending complex scenes.
However, current approaches face challenges in relation modeling: (1) the
representation of semantic prompts is too coarse-grained to capture
fine-grained relationships; (2) these methods lack explicit modeling of image
objects and their semantic relationships. To address these limitations, we
propose RACap, a relation-aware retrieval-augmented model for image captioning,
which not only mines structured relation semantics from retrieval captions, but
also identifies heterogeneous objects from the image. RACap effectively
retrieves structured relation features that contain heterogeneous visual
information to enhance the semantic consistency and relational expressiveness.
Experimental results show that RACap, with only 10.8M trainable parameters,
achieves superior performance compared to previous lightweight captioning
models.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRACapçš„å…³ç³»æ„ŸçŸ¥æ£€ç´¢å¢å¼ºå›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ä»æ£€ç´¢åˆ°çš„æè¿°ä¸­æŒ–æ˜ç»“æ„åŒ–å…³ç³»è¯­ä¹‰å¹¶è¯†åˆ«å›¾åƒä¸­çš„å¼‚æ„å¯¹è±¡ï¼Œä»¥æå‡è¯­ä¹‰ä¸€è‡´æ€§å’Œå…³ç³»è¡¨è¾¾èƒ½åŠ›ï¼Œå®éªŒæ˜¾ç¤ºå…¶åœ¨è½»é‡çº§æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.15577v1">Relevance to Utility: Process-Supervised Rewrite for RAG</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation systems often suffer from a gap between
optimizing retrieval relevance and generative utility: retrieved documents may
be topically relevant but still lack the content needed for effective reasoning
during generation. While existing "bridge" modules attempt to rewrite the
retrieved text for better generation, we show how they fail to capture true
document utility. In this work, we propose R2U, with a key distinction of
directly optimizing to maximize the probability of generating a correct answer
through process supervision. As such direct observation is expensive, we also
propose approximating an efficient distillation pipeline by scaling the
supervision from LLMs, which helps the smaller rewriter model generalize
better. We evaluate our method across multiple open-domain question-answering
benchmarks. The empirical results demonstrate consistent improvements over
strong bridging baselines.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºR2Uçš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³RAGç³»ç»Ÿä¸­æ£€ç´¢ç›¸å…³æ€§ä¸ç”Ÿæˆæ•ˆç”¨ä¹‹é—´çš„ä¸ä¸€è‡´é—®é¢˜ã€‚é€šè¿‡ç›´æ¥ä¼˜åŒ–ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ï¼Œå¹¶åˆ©ç”¨LLMçš„ç›‘ç£ä¿¡å·æ¥é«˜æ•ˆè®­ç»ƒè¾ƒå°çš„é‡å†™æ¨¡å‹ï¼Œè®ºæ–‡åœ¨å¤šä¸ªå¼€æ”¾åŸŸé—®ç­”åŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-18
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.15211v1">What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques</a></td><td><details><summary>å±•å¼€</summary>Slide decks, serving as digital reports that bridge the gap between
presentation slides and written documents, are a prevalent medium for conveying
information in both academic and corporate settings. Their multimodal nature,
combining text, images, and charts, presents challenges for retrieval-augmented
generation systems, where the quality of retrieval directly impacts downstream
performance. Traditional approaches to slide retrieval often involve separate
indexing of modalities, which can increase complexity and lose contextual
information. This paper investigates various methodologies for effective slide
retrieval, including visual late-interaction embedding models like ColPali, the
use of visual rerankers, and hybrid retrieval techniques that combine dense
retrieval with BM25, further enhanced by textual rerankers and fusion methods
like Reciprocal Rank Fusion. A novel Vision-Language Models-based captioning
pipeline is also evaluated, demonstrating significantly reduced embedding
storage requirements compared to visual late-interaction techniques, alongside
comparable retrieval performance. Our analysis extends to the practical aspects
of these methods, evaluating their runtime performance and storage demands
alongside retrieval efficacy, thus offering practical guidance for the
selection and development of efficient and robust slide retrieval systems for
real-world applications.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¹»ç¯ç‰‡ï¼ˆåŒ…å«æ–‡æœ¬ã€å›¾åƒå’Œå›¾è¡¨ï¼‰çš„é«˜æ•ˆæ£€ç´¢æ–¹æ³•ï¼Œæ¢è®¨äº†è§†è§‰å»¶è¿Ÿäº¤äº’åµŒå…¥æ¨¡å‹ã€è§†è§‰é‡æ’åºå™¨ã€æ··åˆæ£€ç´¢æŠ€æœ¯ï¼ˆç»“åˆç¨ å¯†æ£€ç´¢ä¸BM25ï¼‰ç­‰æ–¹æ¡ˆï¼Œå¹¶æå‡ºåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ ‡é¢˜ç”Ÿæˆæµç¨‹ï¼Œåœ¨ä¿è¯æ£€ç´¢æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½å­˜å‚¨éœ€æ±‚ï¼Œä¸ºRAGç³»ç»Ÿä¸­å¹»ç¯ç‰‡æ£€ç´¢çš„å®é™…åº”ç”¨æä¾›æ•ˆèƒ½è¯„ä¼°ä¸å¼€å‘æŒ‡å¯¼ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.15159v1">AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
retrieving relevant documents from external sources to improve factual accuracy
and verifiability. However, this reliance introduces new attack surfaces within
the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have
exposed such vulnerabilities, they largely rely on manipulating user queries,
which is often infeasible in practice due to fixed or protected user inputs.
This narrow focus overlooks a more realistic and stealthy vector: instructional
prompts, which are widely reused, publicly shared, and rarely audited. Their
implicit trust makes them a compelling target for adversaries to manipulate RAG
behavior covertly.
  We introduce a novel attack for Adversarial Instructional Prompt (AIP) that
exploits adversarial instructional prompts to manipulate RAG outputs by subtly
altering retrieval behavior. By shifting the attack surface to the
instructional prompts, AIP reveals how trusted yet seemingly benign interface
components can be weaponized to degrade system integrity. The attack is crafted
to achieve three goals: (1) naturalness, to evade user detection; (2) utility,
to encourage use of prompts; and (3) robustness, to remain effective across
diverse query variations. We propose a diverse query generation strategy that
simulates realistic linguistic variation in user queries, enabling the
discovery of prompts that generalize across paraphrases and rephrasings.
Building on this, a genetic algorithm-based joint optimization is developed to
evolve adversarial prompts by balancing attack success, clean-task utility, and
stealthiness. Experimental results show that AIP achieves up to 95.23% ASR
while preserving benign functionality. These findings uncover a critical and
previously overlooked vulnerability in RAG systems, emphasizing the need to
reassess the shared instructional prompts.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†RAGç³»ç»Ÿä¸­çš„æ–°å‹æ”»å‡»æ–¹å¼Adversarial Instructional Prompt (AIP)ï¼Œé€šè¿‡æ“çºµå¹¿æ³›å¤ç”¨ä¸”æœªè¢«å®¡è®¡çš„æŒ‡ä»¤æç¤ºï¼ˆè€Œéç›´æ¥ç¯¡æ”¹ç”¨æˆ·æŸ¥è¯¢ï¼‰ï¼Œéšç§˜åœ°æ”¹å˜æ£€ç´¢è¡Œä¸ºä»¥æ“æ§è¾“å‡ºã€‚ç ”ç©¶æå‡ºåŸºäºç”Ÿæˆå¤šæ ·æŸ¥è¯¢å’Œé—ä¼ ç®—æ³•çš„è”åˆä¼˜åŒ–æ–¹æ³•ï¼Œæ­ç¤ºRAGä¸­åŸºäºæŒ‡ä»¤æç¤ºçš„å®‰å…¨æ¼æ´ï¼Œå®éªŒæ˜¾ç¤ºAIPæ”»å‡»æˆåŠŸç‡é«˜è¾¾95.23%ä¸”ä¿æŒæ­£å¸¸åŠŸèƒ½ï¼Œå¼ºè°ƒäº†é‡æ–°è¯„ä¼°å…±äº«æç¤ºé£é™©çš„å¿…è¦æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14956v1">Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems</a></td><td><details><summary>å±•å¼€</summary>This paper proposes a novel architectural framework aimed at enhancing
security and reliability in multi-agent systems (MAS). A central component of
this framework is a network of Sentinel Agents, functioning as a distributed
security layer that integrates techniques such as semantic analysis via large
language models (LLMs), behavioral analytics, retrieval-augmented verification,
and cross-agent anomaly detection. Such agents can potentially oversee
inter-agent communications, identify potential threats, enforce privacy and
access controls, and maintain comprehensive audit records. Complementary to the
idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator
Agent supervises policy implementation, and manages agent participation. In
addition, the Coordinator also ingests alerts from Sentinel Agents. Based on
these alerts, it can adapt policies, isolate or quarantine misbehaving agents,
and contain threats to maintain the integrity of the MAS ecosystem. This
dual-layered security approach, combining the continuous monitoring of Sentinel
Agents with the governance functions of Coordinator Agents, supports dynamic
and adaptive defense mechanisms against a range of threats, including prompt
injection, collusive agent behavior, hallucinations generated by LLMs, privacy
breaches, and coordinated multi-agent attacks. In addition to the architectural
design, we present a simulation study where 162 synthetic attacks of different
families (prompt injection, hallucination, and data exfiltration) were injected
into a multi-agent conversational environment. The Sentinel Agents successfully
detected the attack attempts, confirming the practical feasibility of the
proposed monitoring approach. The framework also offers enhanced system
observability, supports regulatory compliance, and enables policy evolution
over time.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¢å¼ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å®‰å…¨æ€§å’Œå¯é æ€§çš„æ–°å‹æ¶æ„æ¡†æ¶ï¼Œå…¶ä¸­åŒ…å«åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè¯­ä¹‰åˆ†æã€æ£€ç´¢å¢å¼ºéªŒè¯ç­‰æŠ€æœ¯ã€‚Sentinel Agentsä½œä¸ºåˆ†å¸ƒå¼å®‰å…¨å±‚ç›‘æ§é€šä¿¡å¹¶è¯†åˆ«å¨èƒï¼ŒCoordinator Agentåˆ™å®æ–½ç­–ç•¥ç®¡ç†å’Œå¨èƒå“åº”ï¼Œå¹¶é€šè¿‡ä»¿çœŸéªŒè¯äº†è¯¥æ¡†æ¶å¯¹æŠ—å¤šç§æ”»å‡»ï¼ˆå¦‚æç¤ºæ³¨å…¥ã€å¹»è§‰ç”Ÿæˆï¼‰çš„æœ‰æ•ˆæ€§ã€‚å…¶æ£€ç´¢å¢å¼ºéªŒè¯ï¼ˆretrieval-augmented verificationï¼‰æŠ€æœ¯æ˜ç¡®ä½“ç°äº†RAGçš„åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14750v1">Enhancing Retrieval Augmentation via Adversarial Collaboration</a></td><td><details><summary>å±•å¼€</summary>Retrieval-augmented Generation (RAG) is a prevalent approach for
domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a
phenomenon where fine-tuned models fail to recognize and act upon poor-quality
retrieved documents, thus undermining performance. To address this, we propose
the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two
heterogeneous agents: a generalist Detector that identifies knowledge gaps, and
a domain-specialized Resolver that provides precise solutions. Guided by a
moderator, these agents engage in an adversarial collaboration, where the
Detector's persistent questioning challenges the Resolver's expertise. This
dynamic process allows for iterative problem dissection and refined knowledge
retrieval. Extensive experiments show that AC-RAG significantly improves
retrieval accuracy and outperforms state-of-the-art RAG methods across various
vertical domains.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºä¸€ç§åä¸ºAC-RAGçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å¯¹æŠ—æ€§åä½œæœºåˆ¶ï¼ˆåŒ…å«é€šç”¨æ£€æµ‹å™¨å’Œé¢†åŸŸä¸“å®¶è§£æå™¨ä¸¤ä¸ªå¼‚æ„ä»£ç†ï¼‰ï¼Œæœ‰æ•ˆè§£å†³RAGä¸­å­˜åœ¨çš„"æ£€ç´¢å¹»è§‰"é—®é¢˜ï¼Œå³æ¨¡å‹æ— æ³•è¯†åˆ«ä½è´¨é‡æ£€ç´¢æ–‡æ¡£çš„ç¼ºé™·ã€‚å®éªŒè¡¨æ˜AC-RAGåœ¨æ£€ç´¢å‡†ç¡®æ€§å’Œå‚ç›´é¢†åŸŸæ€§èƒ½ä¸Šè¶…è¶Šç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14623v1">Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language</a></td><td><details><summary>å±•å¼€</summary>Dynamic energy systems and controls require advanced modeling frameworks to
design and test supervisory and fault tolerant strategies. Modelica is a widely
used equation based language, but developing control modules is labor intensive
and requires specialized expertise. This paper examines the use of large
language models (LLMs) to automate the generation of Control Description
Language modules in the Building Modelica Library as a case study. We developed
a structured workflow that combines standardized prompt scaffolds, library
aware grounding, automated compilation with OpenModelica, and human in the loop
evaluation. Experiments were carried out on four basic logic tasks (And, Or,
Not, and Switch) and five control modules (chiller enable/disable, bypass valve
control, cooling tower fan speed, plant requests, and relief damper control).
The results showed that GPT 4o failed to produce executable Modelica code in
zero shot mode, while Claude Sonnet 4 achieved up to full success for basic
logic blocks with carefully engineered prompts. For control modules, success
rates reached 83 percent, and failed outputs required medium level human repair
(estimated one to eight hours). Retrieval augmented generation often produced
mismatches in module selection (for example, And retrieved as Or), while a
deterministic hard rule search strategy avoided these errors. Human evaluation
also outperformed AI evaluation, since current LLMs cannot assess simulation
results or validate behavioral correctness. Despite these limitations, the LLM
assisted workflow reduced the average development time from 10 to 20 hours down
to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.
These results highlight both the potential and current limitations of LLM
assisted Modelica generation, and point to future research in pre simulation
validation, stronger grounding, and closed loop evaluation.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯è‡ªåŠ¨åŒ–ç”ŸæˆModelicaæ§åˆ¶æ¨¡å—çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆæ ‡å‡†åŒ–æç¤ºæ¡†æ¶ã€åº“æ„ŸçŸ¥åŸºç¡€ã€è‡ªåŠ¨ç¼–è¯‘å’Œäººå·¥è¯„ä¼°ï¼Œæ˜¾è‘—å‡å°‘äº†å¼€å‘æ—¶é—´ï¼ŒåŒæ—¶æŒ‡å‡ºäº†RAGåœ¨æ¨¡å—é€‰æ‹©ä¸Šçš„å±€é™æ€§ä»¥åŠæœªæ¥æ”¹è¿›æ–¹å‘ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14622v1">Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection</a></td><td><details><summary>å±•å¼€</summary>With the deployment of Large Language Models (LLMs) in interactive
applications, online malicious intent detection has become increasingly
critical. However, existing approaches fall short of handling diverse and
complex user queries in real time. To address these challenges, we introduce
ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework
for robust and efficient online malicious intent detection. In the training
stage, a high-capacity teacher model is trained on adversarially perturbed,
retrieval-augmented inputs to learn robust decision boundaries over diverse and
complex user queries. In the inference stage, a distillation scheduler
transfers the teacher's knowledge into a compact student model, with a
continually updated knowledge base collected online. At deployment, the compact
student model leverages top-K similar safety exemplars retrieved from the
online-updated knowledge base to enable both online and real-time malicious
query detection. Evaluations across ten safety benchmarks demonstrate that
ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's
performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on
out-of-distribution detection, while simultaneously delivering up to 5.6x lower
latency at 300 queries per second (QPS) in real-time applications.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ADRAGï¼ˆAdversarial Distilled Retrieval-Augmented Guardï¼‰ï¼Œä¸€ç§ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¯¹æŠ—è’¸é¦çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºå®æ—¶åœ¨çº¿æ¶æ„æ„å›¾æ£€æµ‹ã€‚é€šè¿‡è®­ç»ƒé˜¶æ®µåˆ©ç”¨æ£€ç´¢å¢å¼ºçš„å¯¹æŠ—æ‰°åŠ¨è¾“å…¥è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå°†çŸ¥è¯†è’¸é¦åˆ°è½»é‡çº§å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œå…¶åœ¨çº¿æ›´æ–°çš„çŸ¥è¯†åº“æ”¯æŒå®æ—¶æ£€ç´¢Top-Kç›¸ä¼¼å®‰å…¨ç¤ºä¾‹ï¼Œæ˜¾è‘—æå‡äº†æ¶æ„æŸ¥è¯¢æ£€æµ‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14608v1">Enterprise AI Must Enforce Participant-Aware Access Control</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) are increasingly deployed in enterprise settings
where they interact with multiple users and are trained or fine-tuned on
sensitive internal data. While fine-tuning enhances performance by
internalizing domain knowledge, it also introduces a critical security risk:
leakage of confidential training data to unauthorized users. These risks are
exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG)
pipelines that dynamically fetch contextual documents at inference time.
  We demonstrate data exfiltration attacks on AI assistants where adversaries
can exploit current fine-tuning and RAG architectures to leak sensitive
information by leveraging the lack of access control enforcement. We show that
existing defenses, including prompt sanitization, output filtering, system
isolation, and training-level privacy mechanisms, are fundamentally
probabilistic and fail to offer robust protection against such attacks.
  We take the position that only a deterministic and rigorous enforcement of
fine-grained access control during both fine-tuning and RAG-based inference can
reliably prevent the leakage of sensitive data to unauthorized recipients.
  We introduce a framework centered on the principle that any content used in
training, retrieval, or generation by an LLM is explicitly authorized for
\emph{all users involved in the interaction}. Our approach offers a simple yet
powerful paradigm shift for building secure multi-user LLM systems that are
grounded in classical access control but adapted to the unique challenges of
modern AI workflows. Our solution has been deployed in Microsoft Copilot
Tuning, a product offering that enables organizations to fine-tune models using
their own enterprise-specific data.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡æ–‡ç« æ¢è®¨äº†åœ¨ä¼ä¸šç¯å¢ƒä¸­éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“æ—¶é¢ä¸´çš„æ•°æ®å®‰å…¨é£é™©ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç»†ç²’åº¦è®¿é—®æ§åˆ¶çš„æ¡†æ¶ï¼Œä»¥é˜²æ­¢æ•æ„Ÿä¿¡æ¯æ³„éœ²ç»™æœªç»æˆæƒçš„ç”¨æˆ·ï¼Œå¹¶å·²åœ¨Microsoft Copilot Tuningä¸­éƒ¨ç½²åº”ç”¨ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14507v1">DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction</a></td><td><details><summary>å±•å¼€</summary>Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that
simplifies database access for non-technical users by converting natural
language queries into SQL commands. Recent advancements, particularly those
integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)
reasoning, have made significant strides in enhancing NL2SQL performance.
However, challenges such as inaccurate task decomposition and keyword
extraction by LLMs remain major bottlenecks, often leading to errors in SQL
generation. While existing datasets aim to mitigate these issues by fine-tuning
models, they struggle with over-fragmentation of tasks and lack of
domain-specific keyword annotations, limiting their effectiveness. To address
these limitations, we present DeKeyNLU, a novel dataset which contains 1,500
meticulously annotated QA pairs aimed at refining task decomposition and
enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with
DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three
distinct modules for user question understanding, entity retrieval, and
generation to improve SQL generation accuracy. We benchmarked multiple model
configurations within DeKeySQL RAG pipeline. Experimental results demonstrate
that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy
on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºDeKeyNLUæ•°æ®é›†å’ŒDeKeySQLç®¡é“ï¼Œé€šè¿‡æ”¹è¿›ä»»åŠ¡åˆ†è§£å’Œå…³é”®è¯æå–å¢å¼ºRAGåœ¨è‡ªç„¶è¯­è¨€è½¬SQLï¼ˆNL2SQLï¼‰ä¸­çš„æ€§èƒ½ï¼Œå®éªŒæ˜¾ç¤ºå…¶æ˜¾è‘—æå‡äº†BIRDå’ŒSpideræ•°æ®é›†ä¸Šçš„SQLç”Ÿæˆå‡†ç¡®ç‡ã€‚</details></td></tr></tbody></table>

### ğŸ“… 2025-09-17
<table style='width:100%;'><colgroup><col><col><col></colgroup><thead><tr><th>title</th><th>abstract</th><th>summary</th></tr></thead><tbody><tr><td><a href="http://arxiv.org/abs/2509.14436v1">When Content is Goliath and Algorithm is David: The Style and Semantic Effects of Generative Search Engine</a></td><td><details><summary>å±•å¼€</summary>Generative search engines (GEs) leverage large language models (LLMs) to
deliver AI-generated summaries with website citations, establishing novel
traffic acquisition channels while fundamentally altering the search engine
optimization landscape. To investigate the distinctive characteristics of GEs,
we collect data through interactions with Google's generative and conventional
search platforms, compiling a dataset of approximately ten thousand websites
across both channels. Our empirical analysis reveals that GEs exhibit
preferences for citing content characterized by significantly higher
predictability for underlying LLMs and greater semantic similarity among
selected sources. Through controlled experiments utilizing retrieval augmented
generation (RAG) APIs, we demonstrate that these citation preferences emerge
from intrinsic LLM tendencies to favor content aligned with their generative
expression patterns. Motivated by applications of LLMs to optimize website
content, we conduct additional experimentation to explore how LLM-based content
polishing by website proprietors alters AI summaries, finding that such
polishing paradoxically enhances information diversity within AI summaries.
Finally, to assess the user-end impact of LLM-induced information increases, we
design a generative search engine and recruit Prolific participants to conduct
a randomized controlled experiment involving an information-seeking and writing
task. We find that higher-educated users exhibit minimal changes in their final
outputs' information diversity but demonstrate significantly reduced task
completion time when original sites undergo polishing. Conversely,
lower-educated users primarily benefit through enhanced information density in
their task outputs while maintaining similar completion times across
experimental groups.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡ç ”ç©¶ç”Ÿæˆå¼æœç´¢å¼•æ“ï¼ˆGEsï¼‰çš„ç‰¹ç‚¹åŠå…¶å¼•ç”¨åå¥½ï¼Œå‘ç°GEså€¾å‘äºå¼•ç”¨ä¸åº•å±‚LLMç”Ÿæˆè¡¨è¾¾æ¨¡å¼ä¸€è‡´çš„å†…å®¹ï¼Œå¹¶é€šè¿‡RAG APIå®éªŒéªŒè¯äº†è¿™ä¸€åå¥½æºè‡ªLLMçš„å†…åœ¨å€¾å‘ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†ç½‘ç«™æ‰€æœ‰è€…é€šè¿‡LLMä¼˜åŒ–å†…å®¹å¯¹AIæ‘˜è¦çš„å½±å“ï¼Œå¹¶è¯„ä¼°äº†ä¸åŒæ•™è‚²èƒŒæ™¯ç”¨æˆ·åœ¨ä½¿ç”¨GEsæ—¶çš„è¡¨ç°å·®å¼‚ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.14435v1">Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) have transformed natural language processing
(NLP), enabling diverse applications by integrating large-scale pre-trained
knowledge. However, their static knowledge limits dynamic reasoning over
external information, especially in knowledge-intensive domains.
Retrieval-Augmented Generation (RAG) addresses this challenge by combining
retrieval mechanisms with generative modeling to improve contextual
understanding. Traditional RAG systems suffer from disrupted contextual
integrity due to text chunking and over-reliance on semantic similarity for
retrieval, often resulting in shallow and less accurate responses. We propose
Causal-Counterfactual RAG, a novel framework that integrates explicit causal
graphs representing cause-effect relationships into the retrieval process and
incorporates counterfactual reasoning grounded on the causal structure. Unlike
conventional methods, our framework evaluates not only direct causal evidence
but also the counterfactuality of associated causes, combining results from
both to generate more robust, accurate, and interpretable answers. By
leveraging causal pathways and associated hypothetical scenarios,
Causal-Counterfactual RAG preserves contextual coherence, reduces
hallucination, and enhances reasoning fidelity.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCausal-Counterfactual RAGçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡å°†æ˜¾å¼å› æœå›¾æ•´åˆåˆ°æ£€ç´¢è¿‡ç¨‹ä¸­å¹¶å¼•å…¥åŸºäºå› æœç»“æ„çš„åäº‹å®æ¨ç†ï¼Œè§£å†³äº†ä¼ ç»ŸRAGç³»ç»Ÿå› æ–‡æœ¬åˆ†å—å’Œè¿‡åº¦ä¾èµ–è¯­ä¹‰ç›¸ä¼¼æ€§è€Œå¯¼è‡´çš„ä¸Šä¸‹æ–‡ä¸è¿è´¯å’Œå›ç­”æµ…æ˜¾çš„é—®é¢˜ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®ã€é²æ£’ä¸”å¯è§£é‡Šçš„ç­”æ¡ˆã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.13978v1">LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology</a></td><td><details><summary>å±•å¼€</summary>Modern scientific discovery increasingly relies on workflows that process
data across the Edge, Cloud, and High Performance Computing (HPC) continuum.
Comprehensive and in-depth analyses of these data are critical for hypothesis
validation, anomaly detection, reproducibility, and impactful findings.
Although workflow provenance techniques support such analyses, at large scale,
the provenance data become complex and difficult to analyze. Existing systems
depend on custom scripts, structured queries, or static dashboards, limiting
data interaction. In this work, we introduce an evaluation methodology,
reference architecture, and open-source implementation that leverages
interactive Large Language Model (LLM) agents for runtime data analysis. Our
approach uses a lightweight, metadata-driven design that translates natural
language into structured provenance queries. Evaluations across LLaMA, GPT,
Gemini, and Claude, covering diverse query classes and a real-world chemistry
workflow, show that modular design, prompt tuning, and Retrieval-Augmented
Generation (RAG) enable accurate and insightful LLM agent responses beyond
recorded provenance.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨äº¤äº’å¼å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è¿›è¡Œè¿è¡Œæ—¶æ•°æ®åˆ†æçš„æ–¹æ³•ï¼Œé‡‡ç”¨è½»é‡çº§ã€ä»¥å…ƒæ•°æ®é©±åŠ¨çš„è®¾è®¡å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºç»“æ„åŒ–çš„æº¯æºæŸ¥è¯¢ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å®éªŒï¼ˆæ¶µç›–å¤šç§LLMæ¨¡å‹åŠå®é™…åŒ–å­¦å·¥ä½œæµï¼‰è¯æ˜ï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡ã€æç¤ºè°ƒä¼˜åŠæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯èƒ½æ˜¾è‘—æå‡LLMä»£ç†å“åº”çš„å‡†ç¡®æ€§å’Œæ´å¯ŸåŠ›ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿè®°å½•çš„æº¯æºæ•°æ®èƒ½åŠ›ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.13930v1">Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG</a></td><td><details><summary>å±•å¼€</summary>Multilingual Retrieval-Augmented Generation (mRAG) systems enable language
models to answer knowledge-intensive queries with citation-supported responses
across languages. While such systems have been proposed, an open questions is
whether the mixture of different document languages impacts generation and
citation in unintended ways. To investigate, we introduce a controlled
methodology using model internals to measure language preference while holding
other factors such as document relevance constant. Across eight languages and
six open-weight models, we find that models preferentially cite English sources
when queries are in English, with this bias amplified for lower-resource
languages and for documents positioned mid-context. Crucially, we find that
models sometimes trade-off document relevance for language preference,
indicating that citation choices are not always driven by informativeness
alone. Our findings shed light on how language models leverage multilingual
context and influence citation behavior.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡ç ”ç©¶å¤šè¯­è¨€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆmRAGï¼‰ç³»ç»Ÿä¸­è¯­è¨€åå¥½å¯¹ç”Ÿæˆå’Œå¼•ç”¨çš„å½±å“ï¼Œå‘ç°æ¨¡å‹å€¾å‘äºå¼•ç”¨è‹±æ–‡æ¥æºï¼Œä¸”å¯èƒ½ç‰ºç‰²æ–‡æ¡£ç›¸å…³æ€§è€Œé€‰æ‹©è¯­è¨€åå¥½ï¼Œæ­ç¤ºäº†è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€è¯­å¢ƒä¸­çš„å¼•ç”¨è¡Œä¸ºç‰¹ç‚¹ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.13772v1">Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation</a></td><td><details><summary>å±•å¼€</summary>Retrieval-Augmented Generation (RAG) integrates external knowledge into large
language models to improve response quality. However, recent work has shown
that RAG systems are highly vulnerable to poisoning attacks, where malicious
texts are inserted into the knowledge database to influence model outputs.
While several defenses have been proposed, they are often circumvented by more
adaptive or sophisticated attacks.
  This paper presents RAGOrigin, a black-box responsibility attribution
framework designed to identify which texts in the knowledge database are
responsible for misleading or incorrect generations. Our method constructs a
focused attribution scope tailored to each misgeneration event and assigns a
responsibility score to each candidate text by evaluating its retrieval
ranking, semantic relevance, and influence on the generated response. The
system then isolates poisoned texts using an unsupervised clustering method. We
evaluate RAGOrigin across seven datasets and fifteen poisoning attacks,
including newly developed adaptive poisoning strategies and multi-attacker
scenarios. Our approach outperforms existing baselines in identifying poisoned
content and remains robust under dynamic and noisy conditions. These results
suggest that RAGOrigin provides a practical and effective solution for tracing
the origins of corrupted knowledge in RAG systems.</details></td><td><details><summary>å±•å¼€</summary>æœ¬æ–‡æå‡ºRAGOriginæ¡†æ¶ï¼Œé’ˆå¯¹RAGç³»ç»Ÿä¸­çŸ¥è¯†åº“ä¸­æ¯’æ”»å‡»å¯¼è‡´é”™è¯¯ç”Ÿæˆçš„é—®é¢˜ï¼Œé€šè¿‡é»‘ç›’è´£ä»»æº¯æºæ–¹æ³•åˆ†ææ£€ç´¢æ’åºã€è¯­ä¹‰ç›¸å…³æ€§å’Œç”Ÿæˆå“åº”å½±å“ï¼Œè¯†åˆ«å’Œéš”ç¦»æ¶æ„æ–‡æœ¬ï¼Œå¹¶åœ¨å¤šæ•°æ®é›†å’Œæ”»å‡»åœºæ™¯ä¸‹éªŒè¯å…¶ä¼˜äºç°æœ‰åŸºçº¿ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.13702v1">DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models</a></td><td><details><summary>å±•å¼€</summary>Large Language Model (LLM) hallucination is a significant barrier to their
reliable deployment. Current methods like Retrieval-Augmented Generation (RAG)
are often reactive. We introduce **Dynamic Self-reinforcing Calibration for
Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that
intervenes during autoregressive decoding. Inspired by dual-process cognitive
theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a
Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During
inference, these proxies dynamically steer a large target model by injecting a
real-time steering vector, which is the difference between FAP and HDP logits,
at each decoding step. This plug-and-play approach requires no modification to
the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS
achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%
Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained
the highest FActScore of 46.50. These results validate DSCC-HS as a principled
and efficient solution for enhancing LLM factuality.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDSCC-HSçš„æ–°å‹ä¸»åŠ¨å¼æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è‡ªæˆ‘å¼ºåŒ–æ ¡å‡†æ¥æŠ‘åˆ¶LLMçš„å¹»è§‰é—®é¢˜ï¼Œé‡‡ç”¨åŒä»£ç†æ¨¡å‹ï¼ˆFAPå’ŒHDPï¼‰åœ¨è‡ªå›å½’è§£ç è¿‡ç¨‹ä¸­å®æ—¶ä¿®æ­£ç›®æ ‡æ¨¡å‹çš„è¾“å‡ºã€‚å°½ç®¡å±äºRAGç›¸å…³ç ”ç©¶ï¼ˆæåˆ°RAGä½œä¸ºç°æœ‰æ–¹æ³•å¯¹æ¯”ï¼‰ï¼Œä½†å…¶æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºä¸ä¾èµ–å¤–éƒ¨æ£€ç´¢çš„ä¸»åŠ¨å¹²é¢„æœºåˆ¶ï¼Œå®éªŒè¯æ˜åœ¨TruthfulQAå’ŒBioGENåŸºå‡†ä¸­æ˜¾è‘—æå‡äº†ç”Ÿæˆå†…å®¹çš„çœŸå®æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.13683v1">Improving Context Fidelity via Native Retrieval-Augmented Reasoning</a></td><td><details><summary>å±•å¼€</summary>Large language models (LLMs) often struggle with context fidelity, producing
inconsistent answers when responding to questions based on provided
information. Existing approaches either rely on expensive supervised
fine-tuning to generate evidence post-answer or train models to perform web
searches without necessarily improving utilization of the given context. We
propose CARE, a novel native retrieval-augmented reasoning framework that
teaches LLMs to explicitly integrate in-context evidence within their reasoning
process with the model's own retrieval capabilities. Our method requires
limited labeled evidence data while significantly enhancing both retrieval
accuracy and answer generation performance through strategically retrieved
in-context tokens in the reasoning chain. Extensive experiments on multiple
real-world and counterfactual QA benchmarks demonstrate that our approach
substantially outperforms supervised fine-tuning, traditional
retrieval-augmented generation methods, and external retrieval solutions. This
work represents a fundamental advancement in making LLMs more accurate,
reliable, and efficient for knowledge-intensive tasks.</details></td><td><details><summary>å±•å¼€</summary>è¿™ç¯‡è®ºæ–‡æå‡ºäº†CAREæ¡†æ¶ï¼Œé€šè¿‡è®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ˜¾å¼æ•´åˆä¸Šä¸‹æ–‡è¯æ®å¹¶ç»“åˆè‡ªèº«æ£€ç´¢èƒ½åŠ›ï¼Œæ”¹è¿›äº†ä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢å‡†ç¡®æ€§å’Œç­”æ¡ˆç”Ÿæˆæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¡¹QAåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç›‘ç£å¾®è°ƒå’Œå¤–éƒ¨æ£€ç´¢æ–¹æ¡ˆï¼Œå¢å¼ºäº†LLMsåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</details></td></tr><tr><td><a href="http://arxiv.org/abs/2509.13626v1">Mind the Gap: Aligning Knowledge Bases with User Needs to Enhance Mental Health Retrieval</a></td><td><details><summary>å±•å¼€</summary>Access to reliable mental health information is vital for early help-seeking,
yet expanding knowledge bases is resource-intensive and often misaligned with
user needs. This results in poor performance of retrieval systems when
presented concerns are not covered or expressed in informal or contextualized
language. We present an AI-based gap-informed framework for corpus augmentation
that authentically identifies underrepresented topics (gaps) by overlaying
naturalistic user data such as forum posts in order to prioritize expansions
based on coverage and usefulness. In a case study, we compare Directed
(gap-informed augmentations) with Non-Directed augmentation (random additions),
evaluating the relevance and usefulness of retrieved information across four
retrieval-augmented generation (RAG) pipelines. Directed augmentation achieved
near-optimal performance with modest expansions--requiring only a 42% increase
for Query Transformation, 74% for Reranking and Hierarchical, and 318% for
Baseline--to reach ~95% of the performance of an exhaustive reference corpus.
In contrast, Non-Directed augmentation required substantially larger and thus
practically infeasible expansions to achieve comparable performance (232%,
318%, 403%, and 763%, respectively). These results show that strategically
targeted corpus growth can reduce content creation demands while sustaining
high retrieval and provision quality, offering a scalable approach for building
trusted health information repositories and supporting generative AI
applications in high-stakes domains.</details></td><td><details><summary>å±•å¼€</summary>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºAIçš„æ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«æœªå……åˆ†è¦†ç›–çš„ä¸»é¢˜ï¼ˆç¼ºå£ï¼‰æ¥å¢å¼ºè¯­æ–™åº“ï¼Œå¹¶è¯„ä¼°äº†å…¶åœ¨å››ç§æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“ä¸­çš„æ•ˆæœï¼Œç»“æœæ˜¾ç¤ºå®šå‘å¢å¼ºèƒ½ä»¥è¾ƒå°çš„æ‰©å±•è¾¾åˆ°æ¥è¿‘æœ€ä¼˜çš„æ£€ç´¢æ€§èƒ½ã€‚</details></td></tr></tbody></table>
